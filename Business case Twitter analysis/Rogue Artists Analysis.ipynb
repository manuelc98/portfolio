{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des followers de @Rogue_Artists (Rogue Artists Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installing library for text analytics\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         biography  \\\n",
      "account                                                              \n",
      "thea_touchton    I wrote a poetry book about rape culture and I...   \n",
      "ChicanaChingona  Playwright, poet, writer, director, and chican...   \n",
      "MennaAtPlay      Playwright, director, educator. Gypsy jazz vio...   \n",
      "hey_shawnna      Graphic & UI Designer | Maker Of Things | Coff...   \n",
      "JimmyRGeorge     Screenwriter & Script Consultant @scriptbutche...   \n",
      "\n",
      "                                location  \n",
      "account                                   \n",
      "thea_touchton            Los Angeles, CA  \n",
      "ChicanaChingona            Santa Ana, CA  \n",
      "MennaAtPlay                  Los Angeles  \n",
      "hey_shawnna      South of the Neon Abyss  \n",
      "JimmyRGeorge               Baltimore, MD  \n",
      "There are <bound method DataFrame.count of                                                          biography  \\\n",
      "account                                                              \n",
      "thea_touchton    I wrote a poetry book about rape culture and I...   \n",
      "ChicanaChingona  Playwright, poet, writer, director, and chican...   \n",
      "MennaAtPlay      Playwright, director, educator. Gypsy jazz vio...   \n",
      "hey_shawnna      Graphic & UI Designer | Maker Of Things | Coff...   \n",
      "JimmyRGeorge     Screenwriter & Script Consultant @scriptbutche...   \n",
      "...                                                            ...   \n",
      "thedavidbeach    Actor,  Emcee, Variety Comedian. \\r\\nhttp://t....   \n",
      "MylesNye         Game designer, improviser, puzzler. Survivor f...   \n",
      "tilde                                            Waltzing my self.   \n",
      "ecoalson                        Theatre educator. lover. kittymom.   \n",
      "seancawelti      Artistic Director @Rogue_Artists Director, Vid...   \n",
      "\n",
      "                                location  \n",
      "account                                   \n",
      "thea_touchton            Los Angeles, CA  \n",
      "ChicanaChingona            Santa Ana, CA  \n",
      "MennaAtPlay                  Los Angeles  \n",
      "hey_shawnna      South of the Neon Abyss  \n",
      "JimmyRGeorge               Baltimore, MD  \n",
      "...                                  ...  \n",
      "thedavidbeach     LA.CA ( okay Pasadena)  \n",
      "MylesNye                     Berkeley CA  \n",
      "tilde              34.092341,-118.352883  \n",
      "ecoalson                            Home  \n",
      "seancawelti                  Los Angeles  \n",
      "\n",
      "[2769 rows x 2 columns]> rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "biography    317\n",
       "location     483\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/manuel/Desktop/rogue_followers.csv', index_col=0)\n",
    "df2 = df.copy(deep=True)\n",
    "print(df2.head())\n",
    "print(\"There are \" + str(df2.count) + \" rows\")\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppression des followers aux données incomplètes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account Na : 0\n",
      "bio Na : 0\n",
      "location Na : 0\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.dropna()\n",
    "print('account Na : ' + str(df2.index.isna().sum()))\n",
    "print('bio Na : ' + str(df2.biography.isna().sum()))\n",
    "print('location Na : ' + str(df2.location.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account\n",
      "thea_touchton              Los Angeles, Ca\n",
      "ChicanaChingona              Santa Ana, Ca\n",
      "MennaAtPlay                    Los Angeles\n",
      "hey_shawnna        South Of The Neon Abyss\n",
      "JimmyRGeorge                 Baltimore, Md\n",
      "Name: location, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df2.location = df2.location.str.title()\n",
    "print(df2.location.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top des localisations\n",
    "La moitié des followers réside à Los Angeles où est basé Rogue Artists Ensemble.\n",
    "Si ce n'est à LA, la plupart sont en Californie.\n",
    "Le marché qui montre des signes d'intérêt mais reste peu développé est New York, avec. Il serait intéressant de mieux connaître le profil de ces personnes car New York est reconnu pour son théâtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles             1123\n",
      "California               301\n",
      "New York                 118\n",
      "Hollywood                 91\n",
      "UK                        26\n",
      "Pasadena                  23\n",
      "United States             21\n",
      "San Francisco             15\n",
      "San Diego                 13\n",
      "Phoenix, Az                6\n",
      "Worldwide                  6\n",
      "Global                     6\n",
      "Seattle, Wa                6\n",
      "Washington, Dc             5\n",
      "Austin, Tx                 4\n",
      "Australia                  3\n",
      "Toronto, Ontario           3\n",
      "Earth                      3\n",
      "Massachusetts              3\n",
      "Minneapolis, Mn            3\n",
      "Long Beach                 3\n",
      "City Of Angels             2\n",
      "U.S.A.                     2\n",
      "Vancouver, Bc              2\n",
      "Beverly Hills              2\n",
      "Nashville, Tn              2\n",
      "Colorado Springs, Co       2\n",
      "Charlottesville, Va        2\n",
      "On The Road                2\n",
      "Charlotte, Nc              2\n",
      "Name: location, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cleaning locations to get the rough picture\n",
    "df2.loc[df2.location.str.contains(\"LA\", case=False, regex=False), \"location\"] = \"Los Angeles\"\n",
    "df2.loc[df2.location.str.contains(\"Lost Angeles\", case=False, regex=False), \"location\"] = \"Los Angeles\"\n",
    "df2.loc[df2.location.str.contains(\"L.A.\", case=False, regex=False), \"location\"] = \"Los Angeles\"\n",
    "df2.loc[df2.location.str.contains(\"LOS ANGELES\", case=False, regex=False), \"location\"] = \"Los Angeles\"\n",
    "df2.loc[df2.location.str.contains(\"Nyc\", case=False, regex=False), \"location\"] = \"New York\"\n",
    "df2.loc[df2.location.str.contains(\"New York\", case=False, regex=False), \"location\"] = \"New York\"\n",
    "df2.loc[df2.location.str.contains(\"Ny\", case=False, regex=False), \"location\"] = \"New York\"\n",
    "df2.loc[df2.location.str.contains(\"New York, Usa\", case=False, regex=False), \"location\"] = \"New York\"\n",
    "df2.loc[df2.location.str.contains(\"San Francisco\", case=False, regex=False), \"location\"] = \"San Francisco\"\n",
    "df2.loc[df2.location.str.contains(\"Hollywood\", case=False, regex=False), \"location\"] = \"Hollywood\"\n",
    "df2.loc[df2.location.str.contains(\"uk\", case=False, regex=False), \"location\"] = \"UK\"\n",
    "df2.loc[df2.location.str.contains(\"London\", case=False, regex=False), \"location\"] = \"UK\"\n",
    "df2.loc[df2.location.str.contains(\"San diego\", case=False, regex=False), \"location\"] = \"San Diego\"\n",
    "df2.loc[df2.location.str.contains(\"Santa monica\", case=False, regex=False), \"location\"] = \"Santa Monica\"\n",
    "df2.loc[df2.location.str.contains(\"Pasadena\", case=False, regex=False), \"location\"] = \"Pasadena\"\n",
    "df2.loc[df2.location.str.contains(\"ca\", case=False, regex=False), \"location\"] = \"California\"\n",
    "\n",
    "df3 = df2.location.value_counts()\n",
    "print(df3.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse des bio Twitter : recherche des termes qui ressortent le plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp = df2.biography.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Séparation des phrases en une liste de mots de toutes les bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to break a sentence into a list of words\n",
    "def tokenization(list):\n",
    "    res = []\n",
    "    for x in list:\n",
    "        tokenized_words = word_tokenize(x)\n",
    "        res.append(tokenized_words)\n",
    "    return res\n",
    "\n",
    "# Function to merge the lists from the tokenization\n",
    "def merge(list):\n",
    "    res = []\n",
    "    for x in list:\n",
    "        res = res + x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_biography = tokenization(df_nlp)\n",
    "tokenized_biography = merge(tokenized_biography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Analyse des fréquences des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 10619 samples and 40410 outcomes>\n",
      "[(',', 2606), ('.', 2435), ('and', 896), ('#', 839), ('of', 695), ('the', 648), ('@', 574), (':', 522), ('in', 491), ('&', 443), ('a', 427), ('to', 371), ('!', 347), ('for', 312), ('is', 257), ('I', 245), ('https', 203), ('The', 186), ('|', 174), ('at', 172), ('Theatre', 168), (\"'s\", 166), (')', 160), ('by', 153), ('(', 151), ('theatre', 150), ('Actor', 148), ('-', 143), ('with', 142), ('on', 142)]\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "# Computing the most frequent words, it needs cleaning by withdrowing stop words and punctuation\n",
    "fdist_bio = FreqDist(tokenized_biography)\n",
    "print(fdist_bio)\n",
    "print(fdist_bio.most_common(30))\n",
    "print(fdist_bio.most_common(20)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Suppression des mots inutiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'here', \"shan't\", 'are', 'he', \"don't\", 'now', 'out', \"isn't\", 'through', 'how', 'shan', 'a', 'more', 'doesn', 'because', 'into', 'me', 'against', 'ourselves', 'further', 'hers', \"you'd\", 'an', 'i', \"aren't\", 'their', \"hasn't\", 'up', \"it's\", 'then', 'when', \"should've\", 'doing', 'ma', 'myself', \"weren't\", 'most', 'has', 'them', 'in', 'not', 'll', 'few', 'his', 'who', 'didn', 'over', 'or', 'at', \"doesn't\", 'you', 'such', 'whom', 'having', 'of', 'ours', 'y', 'as', 'nor', 'while', 'our', 'she', 'won', 'did', 'don', 'will', 'off', 'these', \"needn't\", 'the', 'any', 'why', 'ain', 'weren', 'all', 'down', 'wouldn', \"mustn't\", 'there', 're', 'to', 'it', 'if', 'd', \"she's\", 'which', 'should', 'by', \"won't\", 'yours', \"mightn't\", 'only', 'this', 't', 'does', 'was', 'than', 'hadn', 'itself', 'above', 'where', 'but', 'other', 'couldn', 'being', 'hasn', 'haven', 'can', 'mustn', \"you'll\", 'same', \"you're\", 'yourselves', 'its', 'what', \"hadn't\", 'until', 'some', 'those', 'mightn', 'too', 'wasn', 'shouldn', 'him', 've', 'isn', 'on', 'my', 'between', 'with', \"didn't\", 'no', 'o', 'during', 'her', 'about', 'have', 'be', 'just', 'm', 'own', 'theirs', 'been', 'herself', 'before', 'aren', 'under', 'they', 'yourself', 's', 'we', 'each', 'from', 'am', \"wouldn't\", 'once', \"haven't\", \"you've\", 'and', \"that'll\", 'is', 'so', 'were', \"wasn't\", 'that', 'for', 'do', 'themselves', 'again', \"shouldn't\", 'after', 'very', 'below', 'both', 'needn', \"couldn't\", 'himself', 'your'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding stop words according to the stop words we see in Twitter\n",
    "stop_words = ['had', 'here', \"shan't\", 'are', 'he', \"don't\", 'now', 'out', \"isn't\", 'through', 'how', 'shan', 'a', 'more', 'doesn', 'because', 'into', 'me', 'against', 'ourselves', 'further', 'hers', \"you'd\", 'an', 'i', \"aren't\", 'their', \"hasn't\", 'up', \"it's\", 'then', 'when', \"should've\", 'doing', 'ma', 'myself', \"weren't\", 'most', 'has', 'them', 'in', 'not', 'll', 'few', 'his', 'who', 'didn', 'over', 'or', 'at', \"doesn't\", 'you', 'such', 'whom', 'having', 'of', 'ours', 'y', 'as', 'nor', 'while', 'our', 'she', 'won', 'did', 'don', 'will', 'off', 'these', \"needn't\", 'the', 'any', 'why', 'ain', 'weren', 'all', 'down', 'wouldn', \"mustn't\", 'there', 're', 'to', 'it', 'if', 'd', \"she's\", 'which', 'should', 'by', \"won't\", 'yours', \"mightn't\", 'only', 'this', 't', 'does', 'was', 'than', 'hadn', 'itself', 'above', 'where', 'but', 'other', 'couldn', 'being', 'hasn', 'haven', 'can', 'mustn', \"you'll\", 'same', \"you're\", 'yourselves', 'its', 'what', \"hadn't\", 'until', 'some', 'those', 'mightn', 'too', 'wasn', 'shouldn', 'him', 've', 'isn', 'on', 'my', 'between', 'with', \"didn't\", 'no', 'o', 'during', 'her', 'about', 'have', 'be', 'just', 'm', 'own', 'theirs', 'been', 'herself', 'before', 'aren', 'under', 'they', 'yourself', 's', 'we', 'each', 'from', 'am', \"wouldn't\", 'once', \"haven't\", \"you've\", 'and', \"that'll\", 'is', 'so', 'were', \"wasn't\", 'that', 'for', 'do', 'themselves', 'again', \"shouldn't\", 'after', 'very', 'below', 'both', 'needn', \"couldn't\", 'himself', 'your','.', ',', '#', '@', ':', '&', '!', 'I', 'https', 'The', '|', 's', \"'s\", ')', '(', '-', '...', 'Los', 'Angeles', 'A', 'LA','•','’','/',';','http', \"'m\",'?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list with only relevant words by withdrawing stop words\n",
    "filtered_bio = []\n",
    "for w in tokenized_biography:\n",
    "    if w not in stop_words:\n",
    "        filtered_bio.append(w.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mots les plus fréquents dans les bio Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 8813 samples and 23337 outcomes>\n",
      "[('theatre', 318), ('actor', 203), ('director', 161), ('writer', 156), ('new', 147), ('arts', 145), ('artist', 140), ('theater', 139), ('art', 127), ('producer', 105), ('we', 95), ('company', 89), ('love', 78), ('music', 75), ('world', 74), ('artists', 71), ('lover', 70), ('film', 65), ('life', 61), ('actress', 59), ('playwright', 58), ('hollywood', 58), ('community', 57), ('based', 55), ('one', 55), ('us', 54), ('plays', 53), ('professional', 52), ('play', 51), ('comedy', 50), ('musical', 49), ('work', 46), ('creative', 46), ('live', 46), ('designer', 45), ('entertainment', 42), ('make', 41), ('stage', 41), ('things', 40), ('show', 40), ('time', 39), ('shakespeare', 39), ('media', 38), ('like', 38), ('tv', 38), ('people', 37), ('festival', 37), ('theatrical', 36), ('works', 36), ('performing', 35), ('artistic', 34), ('my', 34), ('award-winning', 34), ('ca', 34), ('she/her', 33), ('best', 33), ('follow', 33), ('free', 33), ('california', 32), ('author', 32), (\"'\", 32), ('lathtr', 32), ('performance', 31), (\"n't\", 31), ('social', 31), ('--', 31), ('puppet', 31), ('member', 30), ('teacher', 30), ('creating', 30), ('american', 30), ('since', 29), ('singer', 29), ('acting', 29), ('puppeteer', 29), ('productions', 29), ('actors', 29), ('stories', 28), ('writing', 28), ('dedicated', 28), ('founder', 27), (\"''\", 27), ('dance', 27), ('living', 27), ('fringe', 27), ('family', 26), ('working', 26), ('mom', 26), ('award', 26), ('manager', 26), ('producing', 26), ('maker', 25), ('+', 25), ('all', 25), ('good', 25), ('great', 25), ('production', 25), ('home', 25), ('original', 25), ('city', 25)]\n"
     ]
    }
   ],
   "source": [
    "# We have a first result that can still be homegenized\n",
    "fdist_bio = FreqDist(filtered_bio)\n",
    "print(fdist_bio)\n",
    "print(fdist_bio.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus sur les followers de New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NY = df2[df2.location == \"New York\"]\n",
    "df_NY = df_NY.biography.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_biography = tokenization(df_NY)\n",
    "NY_biography = merge(NY_biography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_NY = []\n",
    "for w in NY_biography:\n",
    "    if w not in stop_words:\n",
    "        filtered_NY.append(w.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bio followers NY\n",
    "Most common themes : theatre, Art, professionnals (director, designer, actor, founder, marketing, management, producer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 19), ('theatre', 12), ('nyc', 10), ('we', 10), ('director', 9), ('theater', 9), ('artist', 8), ('actor', 8), ('art', 8), ('artists', 7), ('broadway', 6), ('producer', 6), ('writer', 6), ('theatrical', 6), ('play', 5), ('friends', 5), ('works', 5), ('playwright', 5), ('professional', 5), ('my', 5), ('one', 5), ('arts', 5), ('puppet', 5), ('world', 5), ('life', 5), ('community', 5), ('york', 5), ('like', 5), ('ny', 4), ('designer', 4), ('board', 4), ('work', 4), ('blacklivesmatter', 4), (\"n't\", 4), ('around', 4), ('music', 4), ('space', 4), ('film', 4), ('plays', 4), ('rock', 3), ('love', 3), ('creative', 3), ('write', 3), ('stage', 3), ('playwrights', 3), ('company', 3), ('founder', 3), ('management', 3), ('marketing', 3), ('creating', 3)]\n"
     ]
    }
   ],
   "source": [
    "fdist_NY = FreqDist(filtered_NY)\n",
    "print(fdist_NY.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus sur les followers de Los Angeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('theatre', 178), ('actor', 146), ('director', 99), ('writer', 98), ('theater', 80), ('new', 78), ('arts', 70), ('artist', 67), ('producer', 66), ('art', 55), ('company', 52), ('music', 41), ('love', 41), ('world', 39), ('lover', 39), ('actress', 38), ('we', 38), ('based', 37), ('musical', 37), ('comedy', 35), ('life', 34), ('playwright', 34), ('hollywood', 34), ('one', 31), ('community', 31), ('film', 31), ('artists', 31), ('plays', 29), ('play', 29), ('us', 29), ('work', 28), ('lathtr', 28), ('shakespeare', 26), ('follow', 26), ('time', 23), ('professional', 23), ('festival', 23), ('works', 23), ('things', 22), ('she/her', 22), ('theatrical', 22), ('creative', 22), ('entertainment', 21), ('stage', 21), ('artistic', 21), ('stories', 21), ('singer', 21), ('author', 19), ('designer', 19), ('living', 19), (\"'\", 19), ('tv', 19), ('award-winning', 19), ('american', 19), ('best', 19), ('people', 18), ('member', 18), ('media', 18), ('free', 18), ('my', 18), ('show', 18), ('live', 18), ('all', 17), ('teacher', 17), ('california', 17), ('voice', 17), ('culture', 16), ('l.a.', 16), ('creator', 16), ('acting', 16), ('he/him', 16), ('fringe', 16), ('actors', 16), ('performing', 16), ('dedicated', 16), ('“', 15), ('filmmaker', 15), ('award', 15), ('experience', 15), ('like', 15), ('an', 15), ('the', 15), ('events', 15), ('around', 15), ('”', 14), ('dance', 14), ('make', 14), ('original', 14), ('dog', 14), ('ca', 14), ('you', 14), ('food', 14), ('home', 14), (\"n't\", 14), ('producing', 14), ('book', 13), ('good', 13), ('bringing', 13), ('education', 13), ('born', 13)]\n"
     ]
    }
   ],
   "source": [
    "df_LA = df2[df2.location == \"Los Angeles\"]\n",
    "df_LA = df_LA.biography.tolist()\n",
    "LA_biography = tokenization(df_LA)\n",
    "LA_biography = merge(LA_biography)\n",
    "\n",
    "filtered_LA = []\n",
    "for w in LA_biography:\n",
    "    if w not in stop_words:\n",
    "        filtered_LA.append(w.lower())\n",
    "        \n",
    "fdist_LA = FreqDist(filtered_LA)\n",
    "print(fdist_LA.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus sur les followers en UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('theatre', 3), ('new', 3), ('founder', 3), ('actor', 3), ('based', 3), ('event', 3), ('producer', 2), ('production', 2), ('independent', 2), ('film', 2), ('us', 2), ('love', 2), ('member', 2), ('product', 2), ('ceo', 2), ('expert', 2), ('arts', 2), ('one', 2), ('director', 2), ('time', 2)]\n"
     ]
    }
   ],
   "source": [
    "df_UK = df2[df2.location == \"UK\"]\n",
    "df_UK = df_UK.biography.tolist()\n",
    "UK_biography = tokenization(df_UK)\n",
    "UK_biography = merge(UK_biography)\n",
    "\n",
    "filtered_UK = []\n",
    "for w in UK_biography:\n",
    "    if w not in stop_words:\n",
    "        filtered_UK.append(w.lower())\n",
    "        \n",
    "fdist_UK = FreqDist(filtered_UK)\n",
    "print(fdist_UK.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
