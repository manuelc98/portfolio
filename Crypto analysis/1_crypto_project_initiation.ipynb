{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3455f9a7-17d6-48d8-8ce1-fce884cfb265",
   "metadata": {},
   "source": [
    "# Crypto scrape and analysis project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff19740-9c3d-4d47-b807-8ac46bac23e3",
   "metadata": {},
   "source": [
    "## Aim of the project\n",
    "- As the cryptocurrency environment is expanding very fast, it has become difficult to gather meaningful information in one place about any project.\n",
    "- Input : a crypto token name (available on CoinGecko)\n",
    "- Outputs : \n",
    "    1. Important data about the token \n",
    "        - price\n",
    "        - price evolution for main periods\n",
    "        - market capitalization\n",
    "        - daily trading volume\n",
    "        - sentiment of people\n",
    "    2. News analysis (using NLP)\n",
    "    3. Designed infography gathering relevant information\n",
    "- Long term goal : evolution and comparison of a portfolio (Ethereum, Solana, Elrond, Uniris...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e8b1e-e782-4b9f-b633-9129a3d8c6e2",
   "metadata": {},
   "source": [
    "## 1. Retrieve CoinGecko's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c750f4aa-acdc-470d-b761-143ccee82cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer #Tokenizer that takes only alphanumerical characters (no punctuation)\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f313cd2f-2449-4796-8b32-c8e6874eeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the % of good sentiment in the sentence scraped\n",
    "# Input : string\n",
    "# Output : int\n",
    "\n",
    "def extract_good_sentiment_pct(sentence):\n",
    "    res = []\n",
    "    for char in sentence:\n",
    "        if char.isdigit():\n",
    "            res.append(char)\n",
    "    n_char = ''\n",
    "    for i in range(len(res)):\n",
    "        n_char += res[i]\n",
    "    n = int(n_char)\n",
    "    return n/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fef6f3e-6890-47b9-a3b8-5d995e065c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the price output of coingecko\n",
    "# Input : unformated price (string)\n",
    "# Output : price (int)\n",
    "\n",
    "def format_price(price_string):\n",
    "    price_string = price_string.replace(\"\\u202f\",\"\")\n",
    "    price_string = price_string.replace(\" $US\",\"\")\n",
    "    price_string = price_string.replace(\",\", \".\")\n",
    "    price = float(price_string)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132692d7-b9eb-473c-a7fa-446302f436ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pct(pct_string):\n",
    "    return(round(float(pct_string.replace('%',''))/100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e27e5cf7-1244-4338-b169-000714abf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_sentiment_button(driver, xpath):\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0,2000)\")\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        system = wait.until(ec.element_to_be_clickable((By.XPATH, xpath)))\n",
    "        system.click()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd1ab55-a464-4f0e-99f4-35cab902372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_search(coin_name):\n",
    "    driver = webdriver.Chrome('/Users/manuel/Documents/GitHub/chromedriver')\n",
    "    driver.get(f\"https://www.coingecko.com/fr/pi%C3%A8ces/{coin_name}\")\n",
    "    driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87fc35f8-2e94-41dd-aed2-e562aa4133a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns coin gecko's data about a given coin\n",
    "# Input : full coin name (ex: bitcoin, ethereum...) (string)\n",
    "# Output : Number of favorites (int), current price in $ (float), \n",
    "\n",
    "def retrieve_coingecko_data(coin_name):\n",
    "    \n",
    "    make_search(coin_name)\n",
    "    \n",
    "    price_string = driver.find_element_by_xpath('/html/body/div[4]/div[4]/div[1]/div/div[1]/div[4]/div/div[1]/span[1]/span').text\n",
    "    price = format_price(price_string)\n",
    "    \n",
    "    hourly_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/span').text\n",
    "    hourly_evolution = format_pct(hourly_evolution_string)\n",
    "    \n",
    "    daily_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[2]/span').text\n",
    "    daily_evolution = format_pct(daily_evolution_string)\n",
    "    \n",
    "    weekly_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[3]/span').text\n",
    "    weekly_evolution = format_pct(weekly_evolution_string)\n",
    "    \n",
    "    bimonthly_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[4]/span').text\n",
    "    bimonthly_evolution = format_pct(bimonthly_evolution_string)\n",
    "    \n",
    "    monthly_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[5]/span').text\n",
    "    monthly_evolution = format_pct(monthly_evolution_string)\n",
    "    \n",
    "    yearly_evolution_string = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[6]/span').text\n",
    "    yearly_evolution = format_pct(yearly_evolution_string)\n",
    "    \n",
    "    market_cap_string = driver.find_element_by_xpath('/html/body/div[4]/div[4]/div[1]/div/div[2]/div[2]/div[1]/div/div/div[1]/span[2]/span').text\n",
    "    market_cap = format_price(market_cap_string)\n",
    "    \n",
    "    daily_trading_string = driver.find_element_by_xpath('/html/body/div[4]/div[4]/div[1]/div/div[2]/div[2]/div[1]/div/div/div[2]/span[2]/span').text\n",
    "    daily_trading = format_price(daily_trading_string)\n",
    "    \n",
    "    sentiment_xpath = \"//*[@id='general']/div[1]/div[1]/div[2]/div[3]/div[1]/div[2]/a[1]\"\n",
    "    click_sentiment_button(driver, sentiment_xpath)\n",
    "    good_sentiment = driver.find_element_by_xpath(\"/html/body/div[4]/div[6]/div/div[2]/div[1]/div/div[1]/div[1]/div[1]/div[2]/div[3]/div[2]/div[2]/div[2]/div[1]\").get_attribute(\"style\")\n",
    "    good_sentiment_n = extract_good_sentiment_pct(good_sentiment)\n",
    "    \n",
    "    return ({\"price ($US)\":price,\n",
    "             \"price_evolution\":{\"1h\":hourly_evolution, \n",
    "                                \"24h\":daily_evolution,\n",
    "                                \"7d\": weekly_evolution,\n",
    "                                \"14d\": bimonthly_evolution,\n",
    "                                \"1m\": monthly_evolution,\n",
    "                                \"1y\": yearly_evolution},\n",
    "             \"market cap\":market_cap,\n",
    "             \"daily trading volume\":daily_trading,\n",
    "             \"good_sentiment\": good_sentiment_n})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96210e51-7a22-4caa-9bb1-a9d88aab0278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price ($US)': 185.9,\n",
       " 'price_evolution': {'1h': -0.007,\n",
       "  '24h': 0.029,\n",
       "  '7d': 0.27,\n",
       "  '14d': 1.113,\n",
       "  '1m': 3.447,\n",
       "  '1y': 52.835},\n",
       " 'market cap': 54569328405.0,\n",
       " 'daily trading volume': 5906181784.0,\n",
       " 'good_sentiment': 0.76}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin=\"solana\"\n",
    "retrieve_coingecko_data(coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14790b7-0876-459d-99b9-3c4c2eb1621c",
   "metadata": {},
   "source": [
    "## News textual analysis (w/ NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5fe20-b6f9-499d-a89e-336146955f35",
   "metadata": {},
   "source": [
    "#### Get 10 last news titles and descriptions related to the coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116a96f9-d2fd-4c08-a5d4-e4e094b75437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_titles(coin_name):\n",
    "    driver = webdriver.Chrome('/Users/manuel/Documents/GitHub/chromedriver')\n",
    "    driver.get(f\"https://www.coingecko.com/fr/pi%C3%A8ces/{coin_name}/news\")\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    titles = driver.find_elements_by_class_name(\"tw-text-xl\")\n",
    "\n",
    "    titles_text = []\n",
    "    for title in titles:\n",
    "        titles_text.append(title.text)\n",
    "    titles_text = titles_text[4:]\n",
    "\n",
    "    descriptions = driver.find_elements_by_class_name(\"post-body\")\n",
    "    desc_text = []\n",
    "    for desc in descriptions:\n",
    "        desc_text.append(desc.text)\n",
    "    \n",
    "    return (titles_text, desc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba726a72-9d3b-4a0f-9ca5-86bee94e7897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Le stablecoin USDC privilégie Solana (SOL) et snobe Ethereum',\n",
       " 'EY choisit Polygon (MATIC) pour mettre à l’échelle ses produits blockchain',\n",
       " '2 milliards de dollars affluent sur Arbitrum… Grâce à Nyan Cat',\n",
       " 'Le géant de l’audit financier EY va s’appuyer sur Polygon pour permettre à ses clients de profiter de l’écosystème Ethereum',\n",
       " 'Les NFTs inspirés de collections Ethereum flambent sur Solana',\n",
       " 'Sacrilège chez OpenSea : des NFT historiques détruits… par erreur',\n",
       " 'Solana est-il un véritable « Ethereum Killer » ? Comparatif et perspectives',\n",
       " 'Investissements institutionnels dans la crypto : les experts pèsent le pour et le contre des implications',\n",
       " 'Les oracles de Chainlink partent à l’assaut d’Optimism sur Ethereum',\n",
       " 'Explosion des utilisateurs du wallet Ethereum Metamask : la barre des 10 millions franchie']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = scrape_news_titles(\"ethereum\")\n",
    "news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d196a-8f4d-4273-b565-1ef684302dc7",
   "metadata": {},
   "source": [
    "#### Tokenize a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c6c943-0ead-4a6f-84f3-ce323348d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input : list of lists\n",
    "#Output : list\n",
    "def concat_list(l):\n",
    "    res = []\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l[i])):\n",
    "            res.append(l[i][j])\n",
    "    return res\n",
    "\n",
    "\n",
    "#Input : list of strings\n",
    "#output : clean list of strings (without spaces and breaks notations)\n",
    "def clean_sentences(l):\n",
    "    clean_list = []\n",
    "    for element in l:\n",
    "        clean_element = element.replace('\\xad', '')\n",
    "        clean_element = clean_element.replace('\\xa0',' ')\n",
    "        clean_list.append(clean_element)\n",
    "    return clean_list\n",
    "\n",
    "\n",
    "#Input : scraped text, as a list of strings (sentences) and a list of stop words to add if necessary\n",
    "#Output : clean list of words of the scraped text (without punctuation or mistakes)\n",
    "def tokenize(text, stop_w_extend):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #set tokenizer to withdraw punctuation\n",
    "    clean_text = clean_sentences(text) #clean text\n",
    "    \n",
    "    sentences_list = []\n",
    "    for element in clean_text: #tokenize each sentence of the text\n",
    "        tokenized_sentence = tokenizer.tokenize(element)\n",
    "        sentences_list.append(tokenized_sentence)\n",
    "        \n",
    "    words_list = concat_list(sentences_list) #make one list of words out of the list of lists of words\n",
    "    words_list = [string.lower() for string in words_list]\n",
    "    \n",
    "    stop_words=set(stopwords.words(\"french\")) #words to withdraw because meaningless\n",
    "    sw_l = [w for w in stop_words]\n",
    "    sw_l.extend(stop_w_extend)\n",
    "\n",
    "    final_words = []\n",
    "    for word in words_list:\n",
    "        if word not in sw_l:\n",
    "            final_words.append(word)\n",
    "\n",
    "    return final_words\n",
    "\n",
    "\n",
    "# Input : text as a list of strings\n",
    "# Output : top n most frequent words in the text\n",
    "def most_frequent_words(text, n, stop_w_extend):\n",
    "    words_list = tokenize(text, stop_w_extend)\n",
    "    return FreqDist(words_list).most_common(n)\n",
    "\n",
    "\n",
    "#Input : text as a list of strings, int for the number of grams, coin_name to spot relevant grams\n",
    "#Output : list of relevant ngrams\n",
    "def word_sequence(text, n, keyword):\n",
    "    sequence = ngrams(text,n)\n",
    "    grams_list = []\n",
    "    for grams in sequence:\n",
    "        if f\"{keyword}\" in grams:\n",
    "              grams_list.append(grams)\n",
    "    return grams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1b834-4ff9-4df0-8521-6cd3663ba24c",
   "metadata": {},
   "source": [
    "## Scrape last 10 articles related to a coin (source : CoinGecko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc159c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract html document from given url\n",
    "def getHTMLdocument(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "  \n",
    "    \n",
    "def scrape_url_text(url):\n",
    "    html_document = getHTMLdocument(url)\n",
    "    soup = BeautifulSoup(html_document, 'html.parser')\n",
    "    text = []\n",
    "    for link in soup.find_all('p'):\n",
    "        text.append(link.text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Input : coin name (string)\n",
    "#Output : text of the last 10 articles on CoinGecko about associated to the coin\n",
    "def scrape_top10_article(coin_name):\n",
    "    driver = webdriver.Chrome('/Users/manuel/Documents/GitHub/chromedriver')\n",
    "    driver.get(f\"https://www.coingecko.com/fr/pi%C3%A8ces/{coin_name}/news\")\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    links = []\n",
    "    for i in range(10):\n",
    "        elem = driver.find_element_by_xpath(f\"//*[@id='news']/article[{i+1}]/div/div[2]/header/h2/a\")\n",
    "        links.append(elem.get_attribute(\"href\"))\n",
    "    \n",
    "    top10_articles = []\n",
    "    for i in range(10):\n",
    "        article_text = scrape_url_text(links[i])\n",
    "        top10_articles.append(article_text)\n",
    "    \n",
    "    top10_articles = concat_list(top10_articles)\n",
    "    \n",
    "    return top10_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b83f9483-ec51-4b81-9265-da79b20cc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw text, not cleaned (one list of strings)\n",
    "top10_test = scrape_top10_article('ethereum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca404f5-304e-4c43-96ee-8486c5830030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and clean the text\n",
    "stop_w = ['a','cette', 'cookies', 'site','to','le', 'la', 'l','of']\n",
    "words_top10_test = tokenize(top10_test, stop_w)\n",
    "\n",
    "# NEXT STEPS :\n",
    "# -> lower words and get all necessary additional stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b975c-c02b-435e-937e-ed98e4d803ce",
   "metadata": {},
   "source": [
    "#### Most frequent words of a text and number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a8d9eb6-f8c3-4a78-b2ae-8c95f78234e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plus', 61),\n",
       " ('ethereum', 59),\n",
       " ('eth', 38),\n",
       " ('réseau', 29),\n",
       " ('2', 28),\n",
       " ('dollars', 25),\n",
       " ('cours', 22),\n",
       " ('milliards', 22),\n",
       " ('bitcoin', 22),\n",
       " ('crypto', 22),\n",
       " ('offre', 19),\n",
       " ('nft', 19),\n",
       " ('3', 18),\n",
       " ('blockchain', 17),\n",
       " ('septembre', 17)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_words(top10_test,15, stop_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948eadf-7864-4b5d-8a34-fe367fb86d41",
   "metadata": {},
   "source": [
    "#### Sequences of n words in which a chosen keyword appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70ea567e-4ff1-4175-bfa0-d9be10a58641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cofondateur', 'réseau', 'ethereum'),\n",
       " ('réseau', 'ethereum', 'vient'),\n",
       " ('ethereum', 'vient', 'temps'),\n",
       " ('cofondateur', 'réseau', 'ethereum'),\n",
       " ('réseau', 'ethereum', 'vient')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sequence(words_top10_test,3, 'ethereum')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3e2c948-642d-4cb3-95d7-5032e14a48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEPS\n",
    "\n",
    "# 1. tokenize\n",
    "# 2. Create a df and attribute weight to meaningful words (out of punctuation and stop words)\n",
    "# 3. Don't forget to uniformize to lower\n",
    "# 4. Loop over sentences, compute their score and filter for less than 30 words sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf5a7a51-94a0-4c17-ac79-f7121dace482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ether</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enregistré</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word\n",
       "0         ether\n",
       "1    enregistré\n",
       "2         nette\n",
       "3  augmentation\n",
       "4         cours"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words_weigth = pd.DataFrame(words_top10_test)\n",
    "df_words_weigth.rename(columns={0:'word'}, inplace = True)\n",
    "\n",
    "df_words_weigth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caed867-179e-4faa-b12c-df5f2c89a7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
